# -*- coding: utf-8 -*-
#"""Loan_Risk_Assessment_Notebook.ipynb

#Automatically generated by Colaboratory.

#Original file is located at
#    https://colab.research.google.com/drive/10HTFsHaUY9K51zXFF6jdC7kXD4t2ciu0
#"""

import pandas as pd
pd.options.display.max_colwidth=150

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
import warnings
warnings.filterwarnings("ignore")
# %matplotlib inline

plt.style.use('fivethirtyeight')

## For making sample data:
from sklearn.datasets import make_classification

## For Preprocessing:
from sklearn.compose import ColumnTransformer,make_column_selector,make_column_transformer
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, RepeatedKFold,RepeatedStratifiedKFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import OrdinalEncoder


# from sklearn.base import TransformerMixin,BaseEstimator

## Using imblearn library:
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline

## Using msno Library for Missing Value analysis:
import missingno as msno

## For Metrics:
from sklearn.metrics import precision_recall_curve,accuracy_score,matthews_corrcoef
from sklearn.metrics import roc_curve, roc_auc_score, auc
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import learning_curve

## For Machine Learning Models:
from sklearn.linear_model import LogisticRegression,LinearRegression
from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

#pip install catboost

from catboost import CatBoostClassifier

## For Pickling:
import pickle

import sklearn
sklearn.__version__

np.random.seed(31415)

df = pd.read_excel(r"C:\Users\WD254XR\OneDrive - EY\Desktop\Generative AI\Use Case - Loan Risk Assessment\Code\res\samples.xlsx", sheet_name ="Samples")
df.head()

df.shape[0],df.shape[1]

df.drop('serial', axis=1, inplace=True)
df.drop('name', axis=1, inplace=True)
df.drop('address', axis=1, inplace=True)
df.drop('SSN', axis=1, inplace=True)

df.drop('phone', axis=1, inplace=True)

df.drop(['loan_int_rate'],axis=1,inplace=True)
df.drop('loan_grade', axis=1, inplace=True)

df.shape

df.info()

ccol=df.select_dtypes(include=["object"]).columns
ncol=df.select_dtypes(include=["int","float"]).columns

print("The number of Categorical columns are:",len(ccol))
print("The number of Numerical columns are:",len(ncol))

df.head()

type(df["loan_status"])

ordinal_encoder = OrdinalEncoder()

df[['loan_status', 'cb_person_default_on_file', 'Existing Loan?']] = ordinal_encoder.fit_transform(df[['loan_status','cb_person_default_on_file', 'Existing Loan?']])

df.head()

df["loan_status"].value_counts(normalize=True)

## Checking for Missing values:
df.isnull().any()

#df.isna().sum()

#msno.matrix(df)

X, X_test, y, y_test = train_test_split(df.drop('loan_status', axis=1), df['loan_status'],
                                        random_state=0,  test_size=0.2, stratify=df['loan_status'],
                                        shuffle=True)

y.value_counts(normalize=True)

y_test.value_counts(normalize=True)

num_pipe = Pipeline([
    ('impute', IterativeImputer()),
    ('scale', StandardScaler()),
])

ct = ColumnTransformer([
    ('num_pipe', num_pipe, make_column_selector(dtype_include=np.number)),
    ('cat_cols', OneHotEncoder(sparse=False, handle_unknown='ignore'), make_column_selector(dtype_include=object))
], remainder='passthrough')


grid = {
   XGBClassifier():
    {'model__n_estimators':[i*100 for i in range(10)],
      'model__max_depth':[6,8,10,12,14,16],
     'model__learning_rate':[0.01, 0.05, 0.1, 0.15, 0.2, 0.3],
     'coltf__num_pipe__impute__estimator':[LinearRegression(), RandomForestRegressor(random_state=0),
                                          KNeighborsRegressor()]},

    LGBMClassifier(class_weight='balanced', random_state=0):
    {'model__n_estimators':[300,400,500],
     'model__learning_rate':[0.001,0.01,0.1,1,10],
     'model__boosting_type': ['gbdt', 'goss', 'dart'],
     'coltf__num_pipe__impute__estimator':[LinearRegression(), RandomForestRegressor(random_state=0),
                                          KNeighborsRegressor()]},

     RandomForestClassifier(random_state=0, class_weight='balanced'):
     {'model__n_estimators':[300,400,500],
      'coltf__num_pipe__impute__estimator': [LinearRegression(), RandomForestRegressor(random_state=0),
                                          KNeighborsRegressor()]},

     KNeighborsClassifier(n_jobs=-1):
     {'model__n_neighbors':[4,5,6,7,8,9],
      'model__weights':['uniform', 'distance'],
      'coltf__num_pipe__impute__estimator':[LinearRegression(), RandomForestRegressor(random_state=0),
                                          KNeighborsRegressor()]}
}

for i,(clf, param) in enumerate(grid.items()):
    print(f"{i+1}. {clf}")
    print(f"\nList of Hyperparameters: {param}")
    print('-'*50)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# full_df = pd.DataFrame()
# best_algos = {}
# 
# X=X[X.select_dtypes(include=[np.number]).columns.append(X.select_dtypes("O").columns)]
# 
# for model, param in grid.items():
# 
#     smt = SMOTE(random_state=42, k_neighbors=2)
# 
#     pipe = Pipeline([
#     ('coltf', ct),
#     ('smote', smt),
#     ('model', model)
# ])
#     print(f"Training {model}!!\n")
#     gs = RandomizedSearchCV(estimator=pipe, param_distributions=param, scoring='accuracy',verbose=3, n_iter=4, random_state=0)
# 
#     print("Fitting!!\n")
#     gs.fit(X, y)
# 
#     print("Gathering Results!!\n")
#     all_res = pd.DataFrame(gs.cv_results_)
# 
#     temp = all_res.loc[:, ['params', 'mean_test_score']]
#     algo_name = str(model).split('(')[0]
#     temp['algo'] = algo_name
# 
#     full_df = pd.concat([full_df, temp], ignore_index=True)
#     best_algos[algo_name] = gs.best_estimator_

full_df.sort_values('mean_test_score', ascending=False)

full_df.sort_values('mean_test_score', ascending=False).iloc[0, 0]

be_xgb = best_algos['XGBClassifier']
be_lgb = best_algos['LGBMClassifier']
be_xgb,be_lgb

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ## A dry run of the best pipeline:
# pipe_xgb = be_xgb
# # evaluate pipeline using k-fold cross validation:
# cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
# scores = cross_val_score(pipe_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
# print(f"The MEAN of score obtained after CROSS VALIDATION of the XGB Based Pipeline is: {scores.mean()} or {scores.mean()*100:.2f}%")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# pipe_xgb.fit(X, y)
# 
# preds_xgb = pipe_xgb.predict(X_test)
# 
# probs_xgb = pipe_xgb.predict_proba(X_test)
# 
# print(f"The ACCURACY SCORE produced on the TEST SET by the XGB Based Pipeline is: {accuracy_score(y_test,preds_xgb)} or {accuracy_score(y_test,preds_xgb)*100}%.")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ## A dry run of the best pipeline:
# pipe_lgb = be_lgb
# # evaluate pipeline using k-fold cross validation:
# cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
# scores = cross_val_score(pipe_lgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
# print(f"The MEAN of score obtained after CROSS VALIDATION of the LGBM Based Pipeline is: {scores.mean()} or {scores.mean()*100:.2f}%")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ## Fitting into best pipeline for evaluation:
# pipe_lgb.fit(X, y)
# ## Getting predictions:
# preds_lgb = pipe_lgb.predict(X_test)
# ## Getting probabilities:
# probs_lgb = pipe_lgb.predict_proba(X_test)
# ## Accuracy Score:
# print(f"The ACCURACY SCORE produced on the TEST SET by the LGBM Based Pipeline is: {accuracy_score(y_test,preds_lgb)} or {accuracy_score(y_test,preds_lgb)*100}%.")

probs_xgb[:10]

probs_lgb[:10]

print(f"The accuracy score of the XGB model is: {accuracy_score(y_test,preds_xgb)}!")
print(f"The accuracy score of the LGB model is: {accuracy_score(y_test,preds_lgb)}!")

import joblib
joblib.dump(pipe_lgb, r"/content/sample_data/best_pipeline.pkl")

